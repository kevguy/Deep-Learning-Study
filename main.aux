\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Probability}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Special Distributions}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Binomial Distribution}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Binomial random variable as a sum of Bernoulli random variables}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Negative Binomial (Pascal) Distribution}{4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Vector Spaces}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Your heading goes here...}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}$R^n$ and $C^n$}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Complex Numbers}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Definition:}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Properties of complex arithmetic}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Definition: \textbf  {\textit  {$-\alpha $, subtraction, $1/\alpha $, division}}}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Notation: $\mathbb  {F}$}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Definition: $\mathbb  {F}^n$}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Definition of Vector Space}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Definition: \textbf  {\textit  {addition, scalar multiplication}}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Definition: \textbf  {\textit  {vector space}}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Definition: \textbf  {\textit  {vector,point}}}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Definition: \textbf  {\textit  {real vector space, complex vector space}}}{9}}
\@writefile{toc}{\contentsline {subsubsection}{Example}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}$\mathbb  {F}^S$}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Unique additive identity}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Unique additive inverse}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.8}The number 0 times a vector}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.9}A number times the vector 0}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.10}The number -1 times a vector}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Subspaces}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Definition: \textbf  {\textit  {subspace}}}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Example:}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Conditions for a subspace}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Sums of Subspaces}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Sum of subspaces is the smallest containing subspace}{13}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Finite-Dimensional Vector Spaces}{14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Span and Linear Independence}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Linear Combination}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}span}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Span is the smallest containing subspace}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}spans}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}finite-dimensional vector space}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}polynomial, $P(\mathbb  {F})$}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.7}degree of a polynomial, $\text  {deg } p$}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.8}$P_m(\mathbb  {F})$}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.9}infinite-dimensional vector space}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.10}Linear Independence}{16}}
\@writefile{toc}{\contentsline {subsubsection}{linearly independent}{16}}
\@writefile{toc}{\contentsline {subsubsection}{linearly dependent}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Linear Dependence Lemma}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Length of linearly independent $\leq $ list length of spanning}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Finite-dimensional subspaces}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bases}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}basis}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Criterion for basis}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Spanning list contains a basis}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Basis of finite-dimensional vector space}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Linearly independent list extends to a basis}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Every subspace of $V$ is a part of a direct sum equal to $V$}{21}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Linear Maps}{22}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Polynomials}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Eigenvalues, Eigenvectors, and Invariant Subspaces}{24}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Inner Product Spaces}{25}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Operators on Inner Product Spaces}{26}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Operators on Complex Vector Spaces}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Operators on Real Vector Spaces}{28}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Trace and Determinant}{29}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Lecture 2: }{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}The dot product}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}The Inner Product as a Decision Rule}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces A picture of a gull.}}{32}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Lecture 3: Perceptron Learning; Maximum Margin Classifiers}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Maximum Margin Classifier}{34}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}DLB - Chapter 3: Probability and Information Theory}{36}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Marginal Probability}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Conditional Probability}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}The Chain Rule of Conditional Probabilities}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Independence and Conditional Independence}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {14.5}Expectation, Variance and Covariance}{37}}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Machine Learning Basics}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}Learning Algorithms}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.1}The Task, $T$}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.2}The Performance Measure, $P$}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.3}The Experience, $E$}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Example: Linear Regression}{42}}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Deep Feedforward Networks}{43}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Convolutional Networks}{45}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}The Convolution Operation}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.1}Convolution and Correlation}{47}}
\@writefile{toc}{\contentsline {subsubsection}{Example:}{47}}
\newlabel{my-label}{{17.1.1}{47}}
\@writefile{lot}{\contentsline {table}{\numberline {17.1}{\ignorespaces image $I$}}{47}}
\newlabel{my-label-1}{{17.1.1}{47}}
\@writefile{lot}{\contentsline {table}{\numberline {17.2}{\ignorespaces image $I$}}{47}}
\newlabel{my-label-2}{{17.1.1}{48}}
\@writefile{lot}{\contentsline {table}{\numberline {17.3}{\ignorespaces image $I$}}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Correlation as a Sliding, Windowed Operation}{48}}
\@writefile{toc}{\contentsline {subsubsection}{A Mathematical Definition for Correlation}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Constructing an Filter from a Continuous Function}{49}}
\@writefile{toc}{\contentsline {subsubsection}{Taking Derivatives with Correlation}{50}}
\newlabel{my-label-3}{{17.1.1}{50}}
\@writefile{lot}{\contentsline {table}{\numberline {17.4}{\ignorespaces image $I$}}{50}}
\newlabel{my-label-4}{{17.1.1}{50}}
\@writefile{lot}{\contentsline {table}{\numberline {17.5}{\ignorespaces image $I$}}{50}}
\@writefile{toc}{\contentsline {subsubsection}{Matching with Correlation}{51}}
\newlabel{my-label-5}{{17.1.1}{51}}
\newlabel{my-label-6}{{17.1.1}{51}}
\newlabel{my-label-7}{{17.1.1}{51}}
\newlabel{my-label-8}{{17.1.1}{52}}
\newlabel{my-label-9}{{17.1.1}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.2}Motivation}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces \textbf  {Sparse Connectivity, view from below}: We highlight one input unit, $x_3$, and highlight the output units in $s$ that are affected by this unit. At the \textbf  {\textit  {top}}, when $s$ is formed by convolution with a kernel of width 3, only three inputs are affected by $x$. At the \textbf  {\textit  {bottom}}, when $s$ is formed by matrix multiplication, connectivity is no longer sparse, so all the outputs are affected by $x_3$.}}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.2}{\ignorespaces \textbf  {Sparse Connectivity, view from above}: We highlight one output unit, $s_3$, and highlight the input units in $x$ that affect this unit. These units are known as the \textbf  {receptive field} of $s_3$. At the \textbf  {\textit  {top}}, when $s$ is formed by convolution with a kernel of width 3, only three inputs affect $s_3$. At the \textbf  {\textit  {bottom}}, when $s$ is formed by matrix multiplication, connectivity is no longer sparse, so all the inputs affect $s_3$.}}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.3}{\ignorespaces The receptive field of the units in the deeper layers of a convolutional network is larger than the receptive field of the units in the shallow layers. This effect increases if the network includes architectural features like \textit  {strided} convolution or \textit  {pooling}. This means that even though \textbf  {\textit  {direct}} connections in a convolutional net are very sparse, units in the deeper layers can be \textbf  {\textit  {indirectly}} connected to all or most of the input image.}}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.4}{\ignorespaces \textbf  {Parameter sharing}: Black arrows indicate the connections that use a particular parameter in two different models. At the \textbf  {\textit  {top}}, the black arrows indicate uses of the central element of a 3-element kernel in a convolutional model.}}{55}}
\newlabel{LastPage}{{}{55}}
\xdef\lastpage@lastpage{55}
\gdef\lastpage@lastpageHy{}
